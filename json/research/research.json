[
    
  {
    "label": ["AINEWs","current"],
    "venues": "CHI2025",
    "info": "CHI2025 Late-breaking Work",
    "imgSrc": "../img/research/AINEWs/AINEWs.jpg",
    "videoSrc":"",
    "imgMore":[""],
    "title": "Combining AI and Crowdsourcing to Identify Misinformation in Online News",
    "link": "",
    "pdf": "",
    "content": "Misinformation in online news presents a critical challenge as it can influence individuals' assessment of news articles and impact decision-making. Existing approaches to combat misinformation rely on either automated AI-based systems or human-centered methods, such as crowdsourcing. However, AI systems often struggle to interpret subjective opinions and remain up to date, while human approaches can be biased. To address these, we designed and developed AI and People News Evaluation (APNE), a system that combines both AI and human assessment to identify misinformation. An initial evaluation of our system suggests that participants valued AI for offering factual information but sometimes felt it was biased. On the other hand, humans, despite having subjective opinions, shared facts and alternative sources that improved the information quality. We also found that participants' assessments were shaped by external factors including their distrust of AI and their opinions about other readers.",
    "abstract": "Misinformation in online news presents a critical challenge as it can influence individuals' assessment of news articles and impact decision-making. Existing approaches to combat misinformation rely on either automated AI-based systems or human-centered methods, such as crowdsourcing. However, AI systems often struggle to interpret subjective opinions and remain up to date, while human approaches can be biased. To address these, we designed and developed AI and People News Evaluation (APNE), a system that combines both AI and human assessment to identify misinformation. An initial evaluation of our system suggests that participants valued AI for offering factual information but sometimes felt it was biased. On the other hand, humans, despite having subjective opinions, shared facts and alternative sources that improved the information quality. We also found that participants' assessments were shaped by external factors including their distrust of AI and their opinions about other readers.",
    "keywords": ["AI"],
    "description": "",
    "url": "",
    "date": "April 29, 2025",
    "researchers": [
        { "name": "Mehrasa Amiri", "link": "" },
        { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
    ],
    "place":"USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": "mamiri2@lsu.edu"
  },
  {
      "label": ["ImmersiveTheNarrative", "current"],
      "venues": "",
      "info": "Innovative Research to Enhance User Immersion",
      "imgSrc": "../img/research/ImmersiveTheNarrative/ImmersiveTheNarrative.webp",
      "videoSrc":"../img/research/ImmersiveTheNarrative/IMG_5127.mp4",
      "imgMore":[""],
      "title": "Application Narrative Visualization for Complex Data Representation and A Visualization-based Critical Incident Survey System",
      "link": "../researchTopic/ImmersiveTheNarrative.html",
      "pdf": "",
      "content": "This research leverages narrative-driven visualizations to effectively present cyberbullying data. By employing data-driven designs, the platform layers and categorizes various forms of bullying (verbal, physical, cyberbullying) and user responses, helping audiences better understand the information. The integration of real-life hybrid physical and digital experiences with data-driven content enhances the immersive experience, allowing users to make informed choices and explore different outcomes. Interactive sound and motion elements further engage users, fostering a deeper connection with the narratives.",
      "abstract": "The project blends hybrid physical-digital experiences with data-driven storytelling, enabling users to explore multiple outcomes through interactive choices. Sound and motion elements amplify immersion, enhancing user engagement with dynamic narratives. It's also including an interactive survey platform that combines critical incident methodology with dynamic visual feedback to elicit more accurate, context-rich responses. Through scenario-driven narratives and reflection prompts, it enhances recall and improves the reliability of self-reported data in complex domains. <br><br>",
      "keywords": ["Narrative visualizations", "Static and animated visualization", "Categorization", "Design space"],
      "description": "",
      "url": "",
      "date": "May, 2024",
      "researchers": [
          { "name": "Vyri Yang", "link": "https://vyriyang.com" },
          { "name": "Fiona Ju", "link": "https://xjcomposer.com" },
          { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
      ],
      "place": "",
      "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
      "school": "Louisiana State University",
      "num": "",
      "email": "jyang44@lsu.edu"
    },
    {
        "label": ["ASL","current"],
        "venues": "CHI2025",
        "info": "CHI2025 Late-breaking Work",
        "imgSrc": "../img/research/ASL/ASL.jpeg",
        "videoSrc":"",
        "imgMore":[""],
        "title": "Voice of the Unheard: Conversational Challenges Between Signers and Non-Signers and Design Interventions for Adaptive SLT Systems",
        "link": "../researchTopic/ASL.html",
        "pdf": "",
        "content": "Signers, like any population, have dialects, variations, and regional modifications, influenced by their choice of signs, speed, and experience. Effective ASL communication occurs when two signers share similar experience and dialects, though ideal scenarios are not always realistic. My research aims to identify communication challenges between signers and non-signers, designing user interfaces that leverage deep learning translation, data augmentation, and person embodiment to minimize these obstacles.",
        "abstract": "Communication barriers remain a significant challenge for the D/deaf community, affecting social interactions, access to services, and overall quality of life. Despite advancements in sign language technology, existing solutions fall short of addressing the unique needs of the D/deaf community. We explore these needs and challenges by conducting a survey with 200 signers and non-signers. Our findings reveal that while the D/deaf community utilizes technology for communication, obstacles including limited translation accuracy, higher costs, and a lack of learning options persist. Additionally, varied proficiency levels, regional dialects, and the lack of visual cues further complicate interactions. We identified key barriers and design approaches that emphasize inclusivity, adaptability, and usability. By incorporating feedback from the D/deaf community, our work provides insights for developing user-centered sign language systems. Our study highlights the importance of bridging the gap between technological innovation and real-world application, paving the way for accessible and transformative communication tools.",
        "keywords": ["ASL"],
        "description": "Another description of the award...",
        "url": "",
        "date": "April 29, 2025",
        "researchers": [
            { "name": "Vimal Joseph", "link": "https://chive.cse.lsu.edu/people/students/VimalThomasJoseph/VimalThomasJoseph.html" },
            { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
        ],
        "place":"USA",
        "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
        "school": "Louisiana State University",
        "num": "",
        "email": "vjosep3@lsu.edu"
      },
    {
      "label": ["ReviewVideo","current"],
      "venues": "CHI2025",
      "info": "CHI2025 Late-breaking Work",
      "imgSrc": "../img/research/ReviewVideo/ReviewVideo.png",
      "videoSrc":"",
      "imgMore":[""],
      "title": "Synoptic: Query-Driven Multimodal Product Review Summarization System",
      "link": "../researchTopic/ReviewVideo.html",
      "pdf": "",
      "content": "This research aims to give an unique user experience to the consumers who watched product review videos. This experience will help them to make a mindful purchase decision and having the insights of the watched videos in their mind.",
      "abstract": "Product reviews play a critical role in making purchase decisions. A recent shift toward multimodal reviews highlights the preference for visual and audio information over texts. While long-form review videos are information-rich, viewers often skip to find what they need, resulting in missing information. Moreover, videos are difficult to search for specific content, leading to ineffective information gathering. In this work, we surveyed 177 people to learn about the challenges and strategies to gather information from product review videos before making purchase decisions. Based on our findings, we designed and developed Synoptic, an interactive multimodal video player that allows users to search within videos and generate video summaries based on customized queries. Our initial evaluation with 40 participants suggests that Synoptic helped viewers gather personalized information from product review videos effectively and efficiently. It also enables purchase decision-making while maintaining user agency and prioritizing their preferences.",
      "keywords": ["ReviewVideo"],
      "description": "",
      "url": "",
      "date": "April 29, 2025",
      "researchers": [
          { "name": "Nushrat Ria", "link": "https://chive.cse.lsu.edu/people/students/NushratJahanRia/nush.html" },
          { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
      ],
      "place":"USA",
      "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
      "school": "Louisiana State University",
      "num": "",
      "email": "nria1@lsu.edu"
    },
    {
    "label": ["LINC", "current"],
    "venues": "",
    "info": "",
    "imgSrc": "../img/research/LINC/LINC.png",
    "videoSrc":"",
    "imgMore":[""],
    "title": "LINC: Language INdependent Collaboration",
    "link": "../researchTopic/LINC.html",
    "pdf": "",
    "content": "Researchers and students from diverse linguistic backgrounds often face challenges in effective communication during academic collaborations. To address these needs, LINC was developed as a multilingual collaboration tool that facilitates communication across languages and provides features for analyzing and reviewing research meetings.",
    "abstract": "At international universities, researchers and students from diverse linguistic backgrounds collaborate on academic projects, often communicating in Englishâ€”a second language for many. These ESL (English as a Second Language) scholars face specific challenges in effectively contributing to and understanding discussions. Through extensive literature review and need-finding surveys, we identified these obstacles, and the functionalities ESLs are looking for in collaborative tools. <br><br>This led to the creation of LINC, an application designed to support efficient, multilingual collaboration during research meetings. LINC allows communication across different languages and offers tools to analyze and review meetings afterward, helping participants contribute and understand the key points discussed.",
    "keywords": ["Case study"],
    "description": "Yet another description of the award...",
    "url": "",
    "date": "Dec, 2024",
    "researchers": [
        { "name": "Saramsh Gautam", "link": "https://chive.cse.lsu.edu/people/students/SaramshGautam/Saramsh.html" },
        { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
    ],
    "place":"USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": "sgauta4@lsu.edu"
  },
  {
    "label": ["AnimatingTheNarrative", "current"],
    "venues": "IEEE VIS2024",
    "info": "VIS2024 Short Papers",
    "imgSrc": "../img/research/AnimatingTheNarrative/vis24b-sub1192-Animating the Narrative-Representative Image.png",
    "videoSrc":"",
    "imgMore":["../img/research/AnimatingTheNarrative/Animating the Narrative Representative Image and Caption.png"],
    "title": "Animating the Narrative: A Review of Animation Styles in Narrative Visualization",
    "link": "../researchTopic/AnimatingTheNarrative.html",
    "pdf": "https://ieeevis.b-cdn.net/vis_2024/pdfs/v-short-1192.pdf",
    "content": "We explore the design space of narrative visualization, focusing on animation styles. We categorize 80 papers from top visualization venues into six categories, including Animation Style, Interactivity, Methodology, Technology, Evaluation Type , and Application Domain. We discuss the interplay between different visualization techniques and elements and the trend to focus on domain-specific visualizations.",
    "abstract": "Narrative visualization has become a crucial tool in data presentation, merging storytelling with data visualization to convey complex information in an engaging and accessible manner. In this study, we review the design space for narrative visualizations, focusing on animation style, through a comprehensive analysis of 80 papers from key visualization venues. We categorize these papers into six broad themes: Animation Style, Interactivity, Technology Usage, Methodology Development, Evaluation Type, and Application Domain. Our findings reveal a significant evolution in the field, marked by a growing preference for animated and non-interactive techniques. This trend reflects a shift towards minimizing user interaction while enhancing the clarity and impact of data presentation. We also identified key trends and technologies shaping the field, highlighting the role of technologies, such as machine learn- ing in driving these changes. We offer insights into the dynamic interrelations within the narrative visualization domains, and suggest future research directions, including exploring non-interactive techniques, examining the interplay between different visualization elements, and developing domain-specific visualizations.<br><br>",
    "keywords": ["Narrative visualizations", "Static and animated visualization", "Categorization", "Design space"],
    "description": "We explore the design space of narrative visualization, focusing on animation styles.",
    "url": "",
    "date": "October 17, 2024",
    "researchers": [
        { "name": "Vyri Yang", "link": "https://vyriyang.com" },
        { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
    ],
    "place": "St. Pete Beach, Florida, USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": "jyang44@lsu.edu"
  },
  {
    "label": ["researchDemo", "last"],
    "venues": "VENUES",
    "info": "VENUES Full Name or any info about your research you wanna show on page",
    "imgSrc": "../img/demo/Demo-5.jpg",
    "videoSrc":"",
    "imgMore":["../img/demo/demo-1.jpeg"],
    "title": "Research Demo",
    "link": "../researchTopic/researchtopic.html",
    "pdf": "https://www.lipsum.com",
    "content": "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.",
    "abstract": "Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of 'de Finibus Bonorum et Malorum' (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, 'Lorem ipsum dolor sit amet..', comes from a line in section 1.10.32.<br><br>The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from 'de Finibus Bonorum et Malorum' by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.<br><br>",
    "keywords": ["research demo"],
    "description": "There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...",
    "url": "https://www.youtube.com/watch?v=YZ84iQrbYjw",
    "date": "June, 2024",
    "researchers": [
        { "name": "Name", "link": "https://chive.cse.lsu.edu/people/DemoPeoplePage/DemoPeoplePage.html" },
        { "name": "Name", "link": "https://chive.cse.lsu.edu/people/DemoPeoplePage/DemoPeoplePage.html" }
    ],
    "place": "place demo, USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": ""
  },

  
  {
      "label": ["current", "last"],
      "imgSrc": "img/explore/explore.png",
      "link": "",
      "title": "View All Research",
      "content": "",
      "date": "",
      "researchers": [
          { "name": "", "link": "" }
      ]
  }
]

