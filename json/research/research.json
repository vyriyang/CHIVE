[
    
  {
    "label": ["AINEWs","current"],
    "venues": "CHI2025",
    "info": "CHI2025 Late-breaking Work",
    "imgSrc": "../img/research/AINEWs/AINEWs.jpg",
    "videoSrc":"",
    "imgMore":["../img/research/AINEWs/AINEWs-1.png","../img/research/AINEWs/AINEWs-2.png","../img/research/AINEWs/AINEWs-3.png"],
    "title": "Combining AI and Crowdsourcing to Identify Misinformation in Online News",
    "link": "../researchTopic/AINEWs.html",
    "pdf": "https://dl.acm.org/doi/pdf/10.1145/3706599.3720007",
    "content": "Misinformation in online news presents a critical challenge as it can influence individuals' assessment of news articles and impact decision-making. Existing approaches to combat misinformation rely on either automated AI-based systems or human-centered methods, such as crowdsourcing. However, AI systems often struggle to interpret subjective opinions and remain up to date, while human approaches can be biased. To address these, we designed and developed AI and People News Evaluation (APNE), a system that combines both AI and human assessment to identify misinformation. An initial evaluation of our system suggests that participants valued AI for offering factual information but sometimes felt it was biased. On the other hand, humans, despite having subjective opinions, shared facts and alternative sources that improved the information quality. We also found that participants' assessments were shaped by external factors including their distrust of AI and their opinions about other readers.",
    "abstract": "Misinformation in online news presents a critical challenge as it can influence individuals' assessment of news articles and impact decision-making. Existing approaches to combat misinformation rely on either automated AI-based systems or human-centered methods, such as crowdsourcing. However, AI systems often struggle to interpret subjective opinions and remain up to date, while human approaches can be biased. To address these, we designed and developed AI and People News Evaluation (APNE), a system that combines both AI and human assessment to identify misinformation. An initial evaluation of our system suggests that participants valued AI for offering factual information but sometimes felt it was biased. On the other hand, humans, despite having subjective opinions, shared facts and alternative sources that improved the information quality. We also found that participants' assessments were shaped by external factors including their distrust of AI and their opinions about other readers.<br><br>",
    "keywords": ["News article","Misinformation","AI","Crowdsourcing"],
    "intro":"In recent years, online news sites have become the dominant source of information with 66% of people in the US getting their news from online news sites [3]. While all news sites boast reliability, assessing the trustworthiness of articles published on these sites remains challenging. Even reputable sources often exhibit biases [21] and perpetuate echo chambers [7], while many other sites are intentionally malicious and misleading [16].<br><br> This issue is particularly critical because many rely on online news to make important decisions. For instance, during the COVID-19 pandemic, conspiracy theories about vaccines spread widely, posing serious risks to public health [17, 22]. Similarly, during the 2024 U.S. elections, numerous biased articles about polls aimed to sway public opinion in specific directions [20]. These examples underscore the urgent need for tools that can help people navigate the complex landscape of online news and distinguish factual information from misinformation. <br><br> Prior works in HCI have proposed several approaches to address the problem of misinformation. Many of these approaches rely on computational methods, such as machine learning algorithms [4] or large language models [26], while others have more human-centered approaches including crowd-sourcing [8]. However, systems that solely rely on human input can be inconsistent, as personal biases often influence assessments [31], and the evaluations on article quality may not always be based on factual evidence [5]. Conversely, AI-based systems, while consistent and scalable, are limited by their inability to make predictions on updated information in real time and challenges in interpreting subjective opinions within news articles [27]. <br><br> In this work, we take a first step toward building systems that can address misinformation by leveraging both crowdsourcing and AI-based approaches. To that end, we designed and developed AI and People News Evaluation (APNE), a system that combines AI’s objectivity alongside humans’ nuanced understanding of subjectivity. This hybrid approach aims to help users make more informed decisions and construct well-founded opinions about current affairs. APNE consists of three main components. The foundational component is the interactive article that allows readers to comment on individual sentences and images. The two complementary components are the AI analysis and the user-generated input sections. Both of these components enable the assessment of the news article based on four key criteria: accuracy, reliability, unbiasedness, and trustworthiness. Additionally, each component provides a textual and visual summary of ratings based on these criteria, offering readers a comprehensive assessment of the article at a glance. To enhance transparency and build trust, both components also provide explanatory details to justify the rating. To evaluate the effectiveness of APNE, we conducted an initial study with 20 participants. Our findings suggest that while AI can provide factual information, it can also propagate bias, and people can provide facts and alternative sources that could better inform other readers. We also found that participants’ opinions on the articles were influenced by factors beyond the article content including source, their mistrust of AI, pre-existing beliefs, and their judgment toward other readers.",
    "description": "",
    "url": "https://youtu.be/NrX6ls3cnXk?si=rshinbuEBcEgkg7w",
    "date": "April 29, 2025",
    "researchers": [
        { "name": "Mehrasa Amiri", "link": "" },
        { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
    ],
    "place":"USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": "mamiri2@lsu.edu"
  },
  {
      "label": ["ImmersiveTheNarrative", "current"],
      "venues": "",
      "info": "Ongoing Research: Innovative Research to Enhance User Immersion",
      "imgSrc": "../img/research/ImmersiveTheNarrative/Poster-Dec12.jpg",
      "videoSrc":"../img/research/ImmersiveTheNarrative/ImmersiveTheNarrative.mp4",
      "imgMore":[""],
      "title": "Application Narrative Visualization for Complex Data Representation",
      "link": "../researchTopic/ImmersiveTheNarrative.html",
      "pdf": "",
      "content": "This research leverages narrative-driven visualizations to effectively present cyberbullying data. By employing data-driven designs, the platform layers and categorizes various forms of bullying (verbal, physical, cyberbullying) and user responses, helping audiences better understand the information. The integration of real-life hybrid physical and digital experiences with data-driven content enhances the immersive experience, allowing users to make informed choices and explore different outcomes. Interactive sound and motion elements further engage users, fostering a deeper connection with the narratives.",
      "abstract": "The project blends hybrid physical-digital experiences with data-driven storytelling, enabling users to explore multiple outcomes through interactive choices. Sound and motion elements amplify immersion, enhancing user engagement with dynamic narratives. It's also including an interactive survey platform that combines critical incident methodology with dynamic visual feedback to elicit more accurate, context-rich responses. Through scenario-driven narratives and reflection prompts, it enhances recall and improves the reliability of self-reported data in complex domains.<br><br>",
      "keywords": ["Narrative visualizations", "Static and animated visualization", "Categorization", "Design space"],
      "intro":"",
      "description": "",
      "url": "",
      "date": "May, 2024",
      "researchers": [
          { "name": "Vyri Yang", "link": "https://vyriyang.com" },
          { "name": "Fiona Ju", "link": "https://xjcomposer.com" },
          { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
      ],
      "place": "",
      "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
      "school": "Louisiana State University",
      "num": "",
      "email": "jyang44@lsu.edu"
    },
    {
        "label": ["ASL","current"],
        "venues": "CHI2025",
        "info": "CHI2025 Late-breaking Work",
        "imgSrc": "../img/research/ASL/ASL.png",
        "videoSrc":"",
        "imgMore":["../img/research/ASL/ASL-1.png"],
        "title": "Voice of the Unheard: Conversational Challenges Between Signers and Non-Signers and Design Interventions for Adaptive SLT Systems",
        "link": "../researchTopic/ASL.html",
        "pdf": "https://dl.acm.org/doi/pdf/10.1145/3706599.3720201",
        "content": "Signers, like any population, have dialects, variations, and regional modifications, influenced by their choice of signs, speed, and experience. Effective ASL communication occurs when two signers share similar experience and dialects, though ideal scenarios are not always realistic. My research aims to identify communication challenges between signers and non-signers, designing user interfaces that leverage deep learning translation, data augmentation, and person embodiment to minimize these obstacles.",
        "abstract": "Communication barriers remain a significant challenge for the D/deaf community, affecting social interactions, access to services, and overall quality of life. Despite advancements in sign language technology, existing solutions fall short of addressing the unique needs of the D/deaf community. We explore these needs and challenges by conducting a survey with 200 signers and non-signers. Our findings reveal that while the D/deaf community utilizes technology for communication, obstacles including limited translation accuracy, higher costs, and a lack of learning options persist. Additionally, varied proficiency levels, regional dialects, and the lack of visual cues further complicate interactions. We identified key barriers and design approaches that emphasize inclusivity, adaptability, and usability. By incorporating feedback from the D/deaf community, our work provides insights for developing user-centered sign language systems. Our study highlights the importance of bridging the gap between technological innovation and real-world application, paving the way for accessible and transformative communication tools.<br><br>",
        "keywords": ["Sign Language Translation", "Accessibility","D/deaf community","Assistive technology","Inclusive design"],
        "intro":"In 2019, 1.57 billion people globally were living with hearing disabilities [16], a number projected to rise to 2.45 billion by 2050 [16]. Alarmingly, hearing loss is increasing among younger populations [31], with approximately 2 to 3 out of every 1,000 children in the U.S. born with hearing loss in one or both ears [9]. Individuals with hearing loss face setbacks spanning from inadequate learning opportunities  [19] to significant employment barriers  [9]. Communication barriers further exacerbate these difficulties, limiting social interactions, access to services, and overall quality of life [9]. For approximately 466 million D/deaf individuals worldwide, sign language [1] is the primary mode of communication [10, 25, 29]. However, issues such as lack of awareness [9], different levels of proficiency [27], and accessibility obstacles often impede effective communication [3], highlighting the urgent need for innovative solutions such as wholistic Sign Language Technology (SLT) systems [14].<br><br> While significant strides have been made in SLT research such as advancements in transformers [6, 10, 34], computer vision [39], neural networks [39], multi-modal translation approaches [20], and wearable technologies [2, 15, 24, 41], existing systems often lack the comprehensiveness required for widespread adoption [11, 14, 36]. Obstacles such as limited datasets [14, 38], variability in sign languages [36], and real-time processing constraints [36] continue to persist. As such, a gap emerges between the development of SLT systems and their effective use by the D/deaf community [1, 35]. Furthermore, the challenges of communicating with the D/deaf community from the perspectives of people not in the same community remain under-explored.<br><br> To address these constraints, we conducted a cross-sectional survey [33] with both signers and non-signers to explore their unique conversational barriers and identify user-focused design requirements. Our findings reveal that while the D/deaf community [1, 35] utilizes technology to aid communication, existing tools are often constrained by limited translation capabilities, higher costs, insufficient learning options, inaccuracy, and lack of versatility and feedback. We also found that signers and non-signers face difficulties in establishing basic communication, even with technological assistance. Alternate communication methods, such as lip reading, hand gestures, and hearing aids, provide some support but are hindered by issues like insufficient resources, background noise, mumbling, lack of eye contact, and limited awareness. Additionally, dialectal and regional variations and signing speeds, contribute to frustration, slowed communication, physical strain, and misinterpretations.<br><br> By highlighting the importance of designing and developing comprehensive SLT systems, we are aligning with the broader goal of advancing human-computer interaction research and adoption in real-world settings [23]. Our study documents the willingness and impediments faced by the D/deaf community in adopting accessible systems and highlights insights for designing user-centered solutions. By engaging the D/deaf community as active contributors, we advocate for systems that prioritize inclusivity, adaptability, and real-world usability. This work presents a significant opportunity to bridge communication gaps and redefine the role of assistive technologies in promoting accessibility.",
        "description": "Another description of the award...",
        "url": "https://youtu.be/rDVsbLsKj8w?si=DbAi5lYNRFOV-VwT",
        "date": "April 29, 2025",
        "researchers": [
            { "name": "Vimal Joseph", "link": "https://chive.cse.lsu.edu/people/students/VimalThomasJoseph/VimalThomasJoseph.html" },
            { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
        ],
        "place":"USA",
        "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
        "school": "Louisiana State University",
        "num": "",
        "email": "vjosep3@lsu.edu"
      },
    {
      "label": ["ReviewVideo","current"],
      "venues": "CHI2025",
      "info": "CHI2025 Late-breaking Work",
      "imgSrc": "../img/research/ReviewVideo/ReviewVideo.png",
      "videoSrc":"",
      "imgMore":["../img/research/ReviewVideo/Synoptic-1.png","../img/research/ReviewVideo/Synoptic-2.png","../img/research/ReviewVideo/Synoptic-3.png"],
      "title": "Synoptic: Query-Driven Multimodal Product Review Summarization System",
      "link": "../researchTopic/ReviewVideo.html",
      "pdf": "https://dl.acm.org/doi/pdf/10.1145/3706599.3719820",
      "content": "This research aims to give an unique user experience to the consumers who watched product review videos. This experience will help them to make a mindful purchase decision and having the insights of the watched videos in their mind.",
      "abstract": "Product reviews play a critical role in making purchase decisions. A recent shift toward multimodal reviews highlights the preference for visual and audio information over texts. While long-form review videos are information-rich, viewers often skip to find what they need, resulting in missing information. Moreover, videos are difficult to search for specific content, leading to ineffective information gathering. In this work, we surveyed 177 people to learn about the challenges and strategies to gather information from product review videos before making purchase decisions. Based on our findings, we designed and developed Synoptic, an interactive multimodal video player that allows users to search within videos and generate video summaries based on customized queries. Our initial evaluation with 40 participants suggests that Synoptic helped viewers gather personalized information from product review videos effectively and efficiently. It also enables purchase decision-making while maintaining user agency and prioritizing their preferences.<br><br>",
      "keywords": ["Product review videos","Video summarization","Interactive transcripts","Multimodal"],
      "intro":"Consumers often consult product reviews to assess the product’s quality, performance, durability, and other attributes to inform their purchase decisions [1, 23, 30, 48]. These reviews are predominantly textual, with occasional images. While beneficial, they often lack comprehensive visual demonstration of products that might impact purchase decisions [36]. Recently, there has been a shift in preference towards multimodal product reviews [3] containing video, audio, and text. This transition has been accelerated by advancements in social media and multimedia platforms that have shifted consumers’ affinity towards multimedia content; making video reviews the preferred method for consumers seeking information-rich product reviews [32]. Prior research suggests that 85% of consumers prefer video reviews over textual ones to gather more comprehensive information before making purchase decisions [13, 17]. However, these product review videos often contain content that may not be sought after by all viewers. For instance, some consumers may be interested in a product’s specifications, while others may prioritize aesthetics or usability [38, 53]. Long-form videos (over 10 minutes [39]) deliver rich content but people seeking targeted information often get lost in the details. While text reviews can be searched for relevant content easily [34], exploring and searching video content is non-trivial [5, 26].<br><br> While existing approaches investigated timestamps [37], subtitles [42], and chapters [55] to enable video exploration, these author-generated markers often fall short of addressing viewers’ preferences for customized content [29, 42, 55]. Some existing approaches enable searching for specific content through transcription [56] with limited success as the visuals may not match the uttered speech. Others have explored interactive technologies employing both crowd-sourcing and machine-learning methods to develop navigation features for video exploration [4, 6, 8, 44]. In addition, video summarization has been explored as an alternative to reduce information-gathering efforts. For instance, Barthel et al. proposed a graph-based summarization method by extracting key frames from the video [9]. Similar approaches include both text-based and frame-based systems to generate meaningful video summaries to help identify essential product details more efficiently without the need to watch the entire video [28, 41, 46, 54]. However, these methods do not support custom queries where users can search the video with specific keywords or questions to retrieve contextual video segments of interest [50]. Existing query-based summarization approaches generate summary videos that are a collection of static image frames [52] or textual summary [35] and not videos by themselves. The lack of an effective way to search for information in videos along with low attention span [14] often results in frustration, dissatisfaction [5] and insufficient information to make confident purchase decisions [30] based on product review videos. Effective alternatives to search for information in review videos remain an under-explored research challenge.<br><br> To better understand how product review videos impact people’s purchase decisions, we conducted a formative study by surveying 177 people. We found that product review videos provide specifications and demonstration of functionalities, enabling more concrete and confident decisions. However, review video length impacts information gathering as long-form videos result in disengagement and short-form videos often lack sufficient information. Participants often skip or fast-forward long-form videos to navigate review videos in a short time. While doing so, they often miss valuable information. We also saw the need for summaries that can help them make quicker decisions.<br><br> Based on the survey responses, we designed and developed Synoptic, a video player that allows users to find information from product review videos using custom queries. We used a large language model (LLM) and a vision language transformer model (ViLT) to identify segments from the video that match the user query and combined them to create the summary videos. Synoptic has two views; the Custom Summary view provides a summary video based on users’ query and the Overlay view presents the complete video, with highlights of the segments that are relevant to the user summary. Finally, we added an interactive transcript and relevant clips as alternative tools for navigating videos. With Synoptic, users can generate customized summary videos based on their information needs and reduce the time needed to gather information from long-form videos. We conducted an initial evaluation of Synoptic with 40 participants, where we asked them to use our system to watch product reviews and make purchase decisions. From the participants’ feedback, we found that Synoptic enabled them to personalize and customize their product review video-watching experience, identify information efficiently, and make confident purchase decisions.",
      "description": "",
      "url": "https://youtu.be/C5uQsA3aS1I?si=BB8JSsBwzYstLM2h",
      "date": "April 29, 2025",
      "researchers": [
          { "name": "Nushrat Ria", "link": "https://chive.cse.lsu.edu/people/students/NushratJahanRia/nush.html" },
          { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
      ],
      "place":"USA",
      "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
      "school": "Louisiana State University",
      "num": "",
      "email": "nria1@lsu.edu"
    },
    {
    "label": ["LINC", "current"],
    "venues": "",
    "info": "Ongoing Research",
    "imgSrc": "../img/research/LINC/LINC.png",
    "videoSrc":"",
    "imgMore":[""],
    "title": "LINC: Language INdependent Collaboration",
    "link": "../researchTopic/LINC.html",
    "pdf": "",
    "content": "Researchers and students from diverse linguistic backgrounds often face challenges in effective communication during academic collaborations. To address these needs, LINC was developed as a multilingual collaboration tool that facilitates communication across languages and provides features for analyzing and reviewing research meetings.",
    "abstract": "At international universities, researchers and students from diverse linguistic backgrounds collaborate on academic projects, often communicating in English—a second language for many. These ESL (English as a Second Language) scholars face specific challenges in effectively contributing to and understanding discussions. Through extensive literature review and need-finding surveys, we identified these obstacles, and the functionalities ESLs are looking for in collaborative tools. <br><br>This led to the creation of LINC, an application designed to support efficient, multilingual collaboration during research meetings. LINC allows communication across different languages and offers tools to analyze and review meetings afterward, helping participants contribute and understand the key points discussed.",
    "keywords": ["Case study"],
    "intro":"",
    "description": "Yet another description of the award...",
    "url": "",
    "date": "Dec, 2024",
    "researchers": [
        { "name": "Saramsh Gautam", "link": "https://chive.cse.lsu.edu/people/students/SaramshGautam/Saramsh.html" },
        { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
    ],
    "place":"USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": "sgauta4@lsu.edu"
  },
  {
    "label": ["AnimatingTheNarrative", "current"],
    "venues": "IEEE VIS2024",
    "info": "VIS2024 Short Papers",
    "imgSrc": "../img/research/AnimatingTheNarrative/vis24b-sub1192-Animating the Narrative-Representative Image.png",
    "videoSrc":"",
    "imgMore":["../img/research/AnimatingTheNarrative/Animating the Narrative Representative Image and Caption.png"],
    "title": "Animating the Narrative: A Review of Animation Styles in Narrative Visualization",
    "link": "../researchTopic/AnimatingTheNarrative.html",
    "pdf": "https://ieeevis.b-cdn.net/vis_2024/pdfs/v-short-1192.pdf",
    "content": "We explore the design space of narrative visualization, focusing on animation styles. We categorize 80 papers from top visualization venues into six categories, including Animation Style, Interactivity, Methodology, Technology, Evaluation Type , and Application Domain. We discuss the interplay between different visualization techniques and elements and the trend to focus on domain-specific visualizations.",
    "abstract": "Narrative visualization has become a crucial tool in data presentation, merging storytelling with data visualization to convey complex information in an engaging and accessible manner. In this study, we review the design space for narrative visualizations, focusing on animation style, through a comprehensive analysis of 80 papers from key visualization venues. We categorize these papers into six broad themes: Animation Style, Interactivity, Technology Usage, Methodology Development, Evaluation Type, and Application Domain. Our findings reveal a significant evolution in the field, marked by a growing preference for animated and non-interactive techniques. This trend reflects a shift towards minimizing user interaction while enhancing the clarity and impact of data presentation. We also identified key trends and technologies shaping the field, highlighting the role of technologies, such as machine learn- ing in driving these changes. We offer insights into the dynamic interrelations within the narrative visualization domains, and suggest future research directions, including exploring non-interactive techniques, examining the interplay between different visualization elements, and developing domain-specific visualizations.<br><br>",
    "keywords": ["Narrative visualizations", "Static and animated visualization", "Categorization", "Design space"],
    "intro":"In the increasingly complex domain of data presentation, narrative visualization has emerged as a vital tool by enhancing communication through rich and engaging data interpretation [15], increasing user engagement with intuitive information representation [25], [20], and improving accessibility with user-friendly designs [25]. It merges storytelling with data visualization, aiming to transform intricate data sets into engaging, informative, and comprehensible stories for a broad audience [14], [35]. Furthermore, narrative visualizations can elicit and establish emotional connections through dynamic and interactive storytelling methods [13], [22], [32]. Recent advancements in technology and visualization techniques have propelled the growth of narrative visualization, particularly through animations [26], [28]. Prior research explored various methods to animate static charts [35], [13] and integrate interactivity [22], significantly enriching the user experience and understanding [19]. These approaches made data representations more dynamic and accessible while introducing new ways to engage and educate viewers.<br><br> Despite the broad usage of animation in narrative visualization, prior works highlighting the importance of animation styles are scattered across various publications and research projects. While some works categorized narrative visualization, many of these studies predominantly focused on narrative intents, organizing data facts, and selecting visual design techniques [34]. Others focused on how different narrative structures can be utilized in data storytelling to affect audience understanding and memory [4]. However, how the animation style interacts with these elements to create a cohesive and effective narrative remains underexplored.<br><br> In this work, we address this gap by categorizing narrative visualization design space, focusing on animation style. We reviewed 80 papers from prominent visualization venues including IEEE Visualization (VIS), Transaction on Visualization and Computer Graphics (TVCG), ACM CHI Conference on Human Factors in Computing Systems, and EuroVIS. We performed multiple reading passes through each collected paper. First, we identified three initial categories based on prior works and our initial reading passes including animation style, workflow, and application domain. Then, we thoroughly reviewed the papers to assign them to initial categories and added new categories following inductive and deductive coding approaches [7]. We refined the emergent categories across several iterations and finalized them into six broad categories: Animation Style, Interactivity, Technology Usage, Methodology Development, Evaluation Type, and Application Domain.<br><br> We further analyzed the papers to identify key trends, technologies, and methodologies using linear regression and Person’s correlation, which led to a deeper understanding of how dynamic and static elements influence narrative effectiveness. Our findings reveal a significant growth in the use of animations in narrative visualizations, with a notable increase in animated and non-interactive techniques. This evolution reflects a trend towards enhancing storytelling and audience engagement while minimizing user interaction. Furthermore, a trend toward survey-centric evaluation for visualizations suggests the importance of user feedback in refining visualization techniques. Our Pearson’s correlation revealed the complementary use of animated and non-interactive elements to enhance user understanding and engagement. Furthermore, we found a strong correlation between animated elements and case studies, which underscores the importance of combining dynamic elements with practical examples to validate theoretical models. Based on these insights, we suggest future research directions, including further exploration of non-interactive techniques, examining the interplay between different visualization elements, and developing domain-specific visualizations tailored to unique user needs.",
    "description": "We explore the design space of narrative visualization, focusing on animation styles.",
    "url": "https://youtu.be/6oCqQbTXScg?si=FgRhI-OQ7rpezu9U",
    "url-2":"https://www.youtube.com/live/7Y2cPfXGiAY?si=U3O9zLpMNboPVIV1&t=4158",
    "date": "October 17, 2024",
    "researchers": [
        { "name": "Vyri Yang", "link": "https://vyriyang.com" },
        { "name": "Mahmood Jasim", "link": "https://csc.lsu.edu/~mjasim/" }
    ],
    "place": "St. Pete Beach, Florida, USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": "jyang44@lsu.edu"
  },
  {
    "label": ["researchDemo", "last"],
    "venues": "VENUES",
    "info": "VENUES Full Name or any info about your research you wanna show on page",
    "imgSrc": "../img/demo/Demo-5.jpg",
    "videoSrc":"",
    "imgMore":["../img/demo/demo-1.jpeg","../img/demo/demo-1.jpeg","../img/demo/demo-1.jpeg"],
    "title": "Research Demo",
    "link": "../researchTopic/researchtopic.html",
    "pdf": "https://www.lipsum.com",
    "content": "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.",
    "abstract": "Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of 'de Finibus Bonorum et Malorum' (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, 'Lorem ipsum dolor sit amet..', comes from a line in section 1.10.32.<br><br>The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from 'de Finibus Bonorum et Malorum' by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.<br><br>",
    "keywords": ["research demo"],
    "intro":"Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source.",
    "description": "There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...",
    "url": "https://www.youtube.com/watch?v=YZ84iQrbYjw",
    "date": "June, 2024",
    "researchers": [
        { "name": "Name", "link": "https://chive.cse.lsu.edu/people/DemoPeoplePage/DemoPeoplePage.html" },
        { "name": "Name", "link": "https://chive.cse.lsu.edu/people/DemoPeoplePage/DemoPeoplePage.html" }
    ],
    "place": "place demo, USA",
    "office": "2329 Patrick F Taylor Building, Baton Rouge, LA 70803",
    "school": "Louisiana State University",
    "num": "",
    "email": ""
  },

  
  {
      "label": ["current", "last"],
      "imgSrc": "img/explore/explore.png",
      "link": "",
      "title": "View All Research",
      "content": "",
      "date": "",
      "researchers": [
          { "name": "", "link": "" }
      ]
  }
]

